{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuHMusOPE9ag"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9d8CGDIJMeN",
        "outputId": "6e375b9c-f67a-494c-9065-176e66108b1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in c:\\users\\jessi\\desktop\\bt4221\\group\\venv-spark\\lib\\site-packages (3.5.5)Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\jessi\\desktop\\bt4221\\group\\venv-spark\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: seaborn in c:\\users\\jessi\\desktop\\bt4221\\group\\venv-spark\\lib\\site-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\jessi\\desktop\\bt4221\\group\\venv-spark\\lib\\site-packages (from seaborn) (2.2.4)\n",
            "Requirement already satisfied: pandas>=1.2 in c:\\users\\jessi\\desktop\\bt4221\\group\\venv-spark\\lib\\site-packages (from seaborn) (2.2.3)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\jessi\\desktop\\bt4221\\group\\venv-spark\\lib\\site-packages (from seaborn) (3.10.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jessi\\desktop\\bt4221\\group\\venv-spark\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\jessi\\desktop\\bt4221\\group\\venv-spark\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jessi\\desktop\\bt4221\\group\\venv-spark\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\jessi\\desktop\\bt4221\\group\\venv-spark\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\jessi\\desktop\\bt4221\\group\\venv-spark\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\jessi\\desktop\\bt4221\\group\\venv-spark\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\jessi\\desktop\\bt4221\\group\\venv-spark\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\jessi\\desktop\\bt4221\\group\\venv-spark\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jessi\\desktop\\bt4221\\group\\venv-spark\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jessi\\desktop\\bt4221\\group\\venv-spark\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\jessi\\desktop\\bt4221\\group\\venv-spark\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: seaborn in c:\\users\\jessi\\desktop\\bt4221\\group\\venv-spark\\lib\\site-packages (0.13.2)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\jessi\\desktop\\bt4221\\group\\venv-spark\\lib\\site-packages (3.10.1)\n",
            "Requirement already satisfied: pandas in c:\\users\\jessi\\desktop\\bt4221\\group\\venv-spark\\lib\\site-packages (2.2.3)\n",
            "Requirement already satisfied: numpy in c:\\users\\jessi\\desktop\\bt4221\\group\\venv-spark\\lib\\site-packages (2.2.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jessi\\desktop\\bt4221\\group\\venv-spark\\lib\\site-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\jessi\\desktop\\bt4221\\group\\venv-spark\\lib\\site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jessi\\desktop\\bt4221\\group\\venv-spark\\lib\\site-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\jessi\\desktop\\bt4221\\group\\venv-spark\\lib\\site-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\jessi\\desktop\\bt4221\\group\\venv-spark\\lib\\site-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\jessi\\desktop\\bt4221\\group\\venv-spark\\lib\\site-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\jessi\\desktop\\bt4221\\group\\venv-spark\\lib\\site-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\jessi\\desktop\\bt4221\\group\\venv-spark\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jessi\\desktop\\bt4221\\group\\venv-spark\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jessi\\desktop\\bt4221\\group\\venv-spark\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\jessi\\desktop\\bt4221\\group\\venv-spark\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark\n",
        "%pip install seaborn\n",
        "%pip install seaborn matplotlib pandas numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wza_1GzTtyur"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import split, col, count, when, sum, expr, udf\n",
        "from pyspark.sql.types import FloatType, DoubleType\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "import math\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHvifmtp9W8I",
        "outputId": "a1fd5426-fc7b-4d24-ee69-e9a5f0644a1c"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"NetTraffic\").getOrCreate()\n",
        "spark.sparkContext.setLogLevel(\"INFO\")\n",
        "\n",
        "print(\"âœ… Spark is running!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+------------------+-------------+---------+--------------+---------+-----+-------+--------+----------+----------+----------+----------+----------+------------+-------+---------+-------------+---------+-------------+--------------+---------------+--------------+\n",
            "|                 ts|               uid|    id.orig_h|id.orig_p|     id.resp_h|id.resp_p|proto|service|duration|orig_bytes|resp_bytes|conn_state|local_orig|local_resp|missed_bytes|history|orig_pkts|orig_ip_bytes|resp_pkts|resp_ip_bytes|tunnel_parents|          label|detailed-label|\n",
            "+-------------------+------------------+-------------+---------+--------------+---------+-----+-------+--------+----------+----------+----------+----------+----------+------------+-------+---------+-------------+---------+-------------+--------------+---------------+--------------+\n",
            "|1.545402842863612E9|CdNmOg26ZIaBRzPvWj|192.168.1.196|  59932.0|104.248.160.24|     80.0|  tcp|      -|3.097754|         0|         0|        S0|         -|         -|         0.0|      S|      3.0|        180.0|      0.0|          0.0|             -|Malicious   C&C|          NULL|\n",
            "|1.545402850041294E9|CgzGV333k9WCximeu8|192.168.1.196|  59932.0|104.248.160.24|     80.0|  tcp|      -|       -|         -|         -|        S0|         -|         -|         0.0|      S|      1.0|         60.0|      0.0|          0.0|             -|Malicious   C&C|          NULL|\n",
            "|1.545402858441479E9|CLm5Pd3ZnqmYVjrZ44|192.168.1.196|  59932.0|104.248.160.24|     80.0|  tcp|      -|       -|         -|         -|        S0|         -|         -|         0.0|      S|      1.0|         60.0|      0.0|          0.0|             -|Malicious   C&C|          NULL|\n",
            "|1.545402853913069E9|CDn2pd1rDD1lCMXAia|192.168.1.196|  35883.0|   192.168.1.1|     53.0|  udp|    dns|5.005148|        78|         0|        S0|         -|         -|         0.0|      D|      2.0|        134.0|      0.0|          0.0|             -|         Benign|             -|\n",
            "| 1.54540284390254E9|C1NKkV3tB4rImzbpDj|192.168.1.196|  43531.0|   192.168.1.1|     53.0|  udp|    dns|5.005145|        78|         0|        S0|         -|         -|         0.0|      D|      2.0|        134.0|      0.0|          0.0|             -|         Benign|             -|\n",
            "+-------------------+------------------+-------------+---------+--------------+---------+-----+-------+--------+----------+----------+----------+----------+----------+------------+-------+---------+-------------+---------+-------------+--------------+---------------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Path to the file (same folder as notebook)\n",
        "file_path = \"dataset_malware.csv\" # change name if needed\n",
        "\n",
        "# Load the CSV into a Spark DataFrame\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True, sep=\"|\")\n",
        "\n",
        "# Show first 5 rows\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7imKjAzND901"
      },
      "source": [
        "# Light Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJExQrJbDw9S"
      },
      "source": [
        "## Display first 5 rows in dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cTmZ3Gygzz4S",
        "outputId": "b967f68f-98ca-4d05-fda5-fd8dfe52cca1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+------------------+-------------+---------+--------------+---------+-----+-------+--------+----------+----------+----------+----------+----------+------------+-------+---------+-------------+---------+-------------+--------------+---------------+--------------+\n",
            "|                 ts|               uid|    id_orig_h|id_orig_p|     id_resp_h|id_resp_p|proto|service|duration|orig_bytes|resp_bytes|conn_state|local_orig|local_resp|missed_bytes|history|orig_pkts|orig_ip_bytes|resp_pkts|resp_ip_bytes|tunnel_parents|          label|detailed-label|\n",
            "+-------------------+------------------+-------------+---------+--------------+---------+-----+-------+--------+----------+----------+----------+----------+----------+------------+-------+---------+-------------+---------+-------------+--------------+---------------+--------------+\n",
            "|1.545402842863612E9|CdNmOg26ZIaBRzPvWj|192.168.1.196|  59932.0|104.248.160.24|     80.0|  tcp|   NULL|3.097754|       0.0|       0.0|        S0|      NULL|      NULL|         0.0|      S|      3.0|        180.0|      0.0|          0.0|          NULL|Malicious   C&C|          NULL|\n",
            "|1.545402850041294E9|CgzGV333k9WCximeu8|192.168.1.196|  59932.0|104.248.160.24|     80.0|  tcp|   NULL|    NULL|      NULL|      NULL|        S0|      NULL|      NULL|         0.0|      S|      1.0|         60.0|      0.0|          0.0|          NULL|Malicious   C&C|          NULL|\n",
            "|1.545402858441479E9|CLm5Pd3ZnqmYVjrZ44|192.168.1.196|  59932.0|104.248.160.24|     80.0|  tcp|   NULL|    NULL|      NULL|      NULL|        S0|      NULL|      NULL|         0.0|      S|      1.0|         60.0|      0.0|          0.0|          NULL|Malicious   C&C|          NULL|\n",
            "|1.545402853913069E9|CDn2pd1rDD1lCMXAia|192.168.1.196|  35883.0|   192.168.1.1|     53.0|  udp|    dns|5.005148|      78.0|       0.0|        S0|      NULL|      NULL|         0.0|      D|      2.0|        134.0|      0.0|          0.0|          NULL|         Benign|          NULL|\n",
            "| 1.54540284390254E9|C1NKkV3tB4rImzbpDj|192.168.1.196|  43531.0|   192.168.1.1|     53.0|  udp|    dns|5.005145|      78.0|       0.0|        S0|      NULL|      NULL|         0.0|      D|      2.0|        134.0|      0.0|          0.0|          NULL|         Benign|          NULL|\n",
            "+-------------------+------------------+-------------+---------+--------------+---------+-----+-------+--------+----------+----------+----------+----------+----------+------------+-------+---------+-------------+---------+-------------+--------------+---------------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# print(df_split.head())\n",
        "# Show first 5 rows\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlXAE87KCnu9"
      },
      "source": [
        "## Show schema to identify column types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ocADM6cYz2kk",
        "outputId": "52e67fb5-554c-4b7f-a1ba-105e697feebc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- ts: double (nullable = true)\n",
            " |-- uid: string (nullable = true)\n",
            " |-- id_orig_h: string (nullable = true)\n",
            " |-- id_orig_p: float (nullable = true)\n",
            " |-- id_resp_h: string (nullable = true)\n",
            " |-- id_resp_p: float (nullable = true)\n",
            " |-- proto: string (nullable = true)\n",
            " |-- service: string (nullable = true)\n",
            " |-- duration: float (nullable = true)\n",
            " |-- orig_bytes: float (nullable = true)\n",
            " |-- resp_bytes: float (nullable = true)\n",
            " |-- conn_state: string (nullable = true)\n",
            " |-- local_orig: string (nullable = true)\n",
            " |-- local_resp: string (nullable = true)\n",
            " |-- missed_bytes: float (nullable = true)\n",
            " |-- history: string (nullable = true)\n",
            " |-- orig_pkts: float (nullable = true)\n",
            " |-- orig_ip_bytes: float (nullable = true)\n",
            " |-- resp_pkts: float (nullable = true)\n",
            " |-- resp_ip_bytes: float (nullable = true)\n",
            " |-- tunnel_parents: string (nullable = true)\n",
            " |-- label: string (nullable = true)\n",
            " |-- detailed-label: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check number of rows and columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ§¾ Number of rows: 10447787\n",
            "ðŸ§¾ Number of columns: 23\n"
          ]
        }
      ],
      "source": [
        "num_rows = df.count()\n",
        "print(f\"ðŸ§¾ Number of rows: {num_rows}\")\n",
        "num_columns = len(df.columns)\n",
        "print(f\"ðŸ§¾ Number of columns: {num_columns}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oE2c8Gp4RIIM"
      },
      "source": [
        "## Cast '-' to NaN change column names so that Spark will not misinterpret them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GUlLAcOpRITz",
        "outputId": "2aa93cba-8a1e-41eb-b1fc-58f440aa63a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+------------------+-------------+---------+--------------+---------+-----+-------+--------+----------+----------+----------+----------+----------+------------+-------+---------+-------------+---------+-------------+--------------+---------------+--------------+\n",
            "|                 ts|               uid|    id_orig_h|id_orig_p|     id_resp_h|id_resp_p|proto|service|duration|orig_bytes|resp_bytes|conn_state|local_orig|local_resp|missed_bytes|history|orig_pkts|orig_ip_bytes|resp_pkts|resp_ip_bytes|tunnel_parents|          label|detailed-label|\n",
            "+-------------------+------------------+-------------+---------+--------------+---------+-----+-------+--------+----------+----------+----------+----------+----------+------------+-------+---------+-------------+---------+-------------+--------------+---------------+--------------+\n",
            "|1.545402842863612E9|CdNmOg26ZIaBRzPvWj|192.168.1.196|  59932.0|104.248.160.24|     80.0|  tcp|   NULL|3.097754|         0|         0|        S0|      NULL|      NULL|         0.0|      S|      3.0|        180.0|      0.0|          0.0|          NULL|Malicious   C&C|          NULL|\n",
            "|1.545402850041294E9|CgzGV333k9WCximeu8|192.168.1.196|  59932.0|104.248.160.24|     80.0|  tcp|   NULL|    NULL|      NULL|      NULL|        S0|      NULL|      NULL|         0.0|      S|      1.0|         60.0|      0.0|          0.0|          NULL|Malicious   C&C|          NULL|\n",
            "|1.545402858441479E9|CLm5Pd3ZnqmYVjrZ44|192.168.1.196|  59932.0|104.248.160.24|     80.0|  tcp|   NULL|    NULL|      NULL|      NULL|        S0|      NULL|      NULL|         0.0|      S|      1.0|         60.0|      0.0|          0.0|          NULL|Malicious   C&C|          NULL|\n",
            "|1.545402853913069E9|CDn2pd1rDD1lCMXAia|192.168.1.196|  35883.0|   192.168.1.1|     53.0|  udp|    dns|5.005148|        78|         0|        S0|      NULL|      NULL|         0.0|      D|      2.0|        134.0|      0.0|          0.0|          NULL|         Benign|          NULL|\n",
            "| 1.54540284390254E9|C1NKkV3tB4rImzbpDj|192.168.1.196|  43531.0|   192.168.1.1|     53.0|  udp|    dns|5.005145|        78|         0|        S0|      NULL|      NULL|         0.0|      D|      2.0|        134.0|      0.0|          0.0|          NULL|         Benign|          NULL|\n",
            "+-------------------+------------------+-------------+---------+--------------+---------+-----+-------+--------+----------+----------+----------+----------+----------+------------+-------+---------+-------------+---------+-------------+--------------+---------------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "for column in df.columns:\n",
        "    df = df.withColumn(column, when(col(f\"`{column}`\") == \"-\", None).otherwise(col(f\"`{column}`\")))\n",
        "\n",
        "# Rename columns to replace dots with underscores\n",
        "new_column_names = [c.replace(\".\", \"_\") for c in df.columns]\n",
        "df = df.toDF(*new_column_names)\n",
        "\n",
        "# Check\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dealing with missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ts</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>uid</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id_orig_h</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id_orig_p</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id_resp_h</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id_resp_p</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>proto</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>service</th>\n",
              "      <td>10446261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>duration</th>\n",
              "      <td>4432615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>orig_bytes</th>\n",
              "      <td>4432615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>resp_bytes</th>\n",
              "      <td>4432615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>conn_state</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>local_orig</th>\n",
              "      <td>10447787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>local_resp</th>\n",
              "      <td>10447787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>missed_bytes</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>history</th>\n",
              "      <td>1638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>orig_pkts</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>orig_ip_bytes</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>resp_pkts</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>resp_ip_bytes</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tunnel_parents</th>\n",
              "      <td>10447787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>detailed-label</th>\n",
              "      <td>10447775</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   Count\n",
              "ts                     0\n",
              "uid                    0\n",
              "id_orig_h              0\n",
              "id_orig_p              0\n",
              "id_resp_h              0\n",
              "id_resp_p              0\n",
              "proto                  0\n",
              "service         10446261\n",
              "duration         4432615\n",
              "orig_bytes       4432615\n",
              "resp_bytes       4432615\n",
              "conn_state             0\n",
              "local_orig      10447787\n",
              "local_resp      10447787\n",
              "missed_bytes           0\n",
              "history             1638\n",
              "orig_pkts              0\n",
              "orig_ip_bytes          0\n",
              "resp_pkts              0\n",
              "resp_ip_bytes          0\n",
              "tunnel_parents  10447787\n",
              "label                  0\n",
              "detailed-label  10447775"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, sum as spark_sum\n",
        "\n",
        "null_values = (\n",
        "    df.select([\n",
        "        spark_sum(col(c).isNull().cast(\"int\")).alias(c)\n",
        "        for c in df.columns\n",
        "    ])\n",
        "    .toPandas()\n",
        "    .T.rename(columns={0: \"Count\"})\n",
        ")\n",
        "\n",
        "display(null_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since duration, orig_bytes, resp_bytes all have same number of null values (4432615), very likely they are from the same few rows. All three columns are critical for modelling so it is best to drop the rows containing null values in those fields.\n",
        "\n",
        "Others like service, local_orig, local_resp, tunnel_parents amd detailed_label all have more than 10446261 out of 10447787 null values. These will not help our model and will just bloat memory, so we can drop these columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ts</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>uid</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id_orig_h</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id_orig_p</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id_resp_h</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id_resp_p</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>proto</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>service</th>\n",
              "      <td>6013804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>duration</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>orig_bytes</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>resp_bytes</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>conn_state</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>local_orig</th>\n",
              "      <td>6015172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>local_resp</th>\n",
              "      <td>6015172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>missed_bytes</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>history</th>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>orig_pkts</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>orig_ip_bytes</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>resp_pkts</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>resp_ip_bytes</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tunnel_parents</th>\n",
              "      <td>6015172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>detailed-label</th>\n",
              "      <td>6015160</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Count\n",
              "ts                    0\n",
              "uid                   0\n",
              "id_orig_h             0\n",
              "id_orig_p             0\n",
              "id_resp_h             0\n",
              "id_resp_p             0\n",
              "proto                 0\n",
              "service         6013804\n",
              "duration              0\n",
              "orig_bytes            0\n",
              "resp_bytes            0\n",
              "conn_state            0\n",
              "local_orig      6015172\n",
              "local_resp      6015172\n",
              "missed_bytes          0\n",
              "history              22\n",
              "orig_pkts             0\n",
              "orig_ip_bytes         0\n",
              "resp_pkts             0\n",
              "resp_ip_bytes         0\n",
              "tunnel_parents  6015172\n",
              "label                 0\n",
              "detailed-label  6015160"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from pyspark.sql.functions import when, col\n",
        "\n",
        "# Columns you care about\n",
        "columns_to_clean = ['duration', 'orig_bytes', 'resp_bytes']\n",
        "\n",
        "# Replace \"NULL\" or \"null\" strings with actual nulls (None)\n",
        "for c in columns_to_clean:\n",
        "    df = df.withColumn(c, when((col(c) == \"NULL\") | (col(c) == \"null\"), None).otherwise(col(c)))\n",
        "\n",
        "# Now drop rows where any of these are null\n",
        "df = df.dropna(subset=columns_to_clean)\n",
        "\n",
        "from pyspark.sql.functions import col, sum as spark_sum\n",
        "\n",
        "# now drop columns with largely null values\n",
        "columns_to_drop = [\"service\", \"local_orig\", \"local_resp\", \"tunnel_parents\", \"missed_bytes\"]\n",
        "df = df.drop(*columns_to_drop)\n",
        "\n",
        "\n",
        "null_values = (\n",
        "    df.select([\n",
        "        spark_sum(col(c).isNull().cast(\"int\")).alias(c)\n",
        "        for c in df.columns\n",
        "    ])\n",
        "    .toPandas()\n",
        "    .T.rename(columns={0: \"Count\"})\n",
        ")\n",
        "\n",
        "display(null_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "They are indeed from the same few rows. now there are 0 NULL count for duration, orig_bytes and resp_bytes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## check number of rows and colums after dropping rows with null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ§¾ Number of rows: 6015172\n",
            "ðŸ§¾ Number of columns: 23\n"
          ]
        }
      ],
      "source": [
        "num_rows = df.count()\n",
        "print(f\"ðŸ§¾ Number of rows: {num_rows}\")\n",
        "num_columns = len(df.columns)\n",
        "print(f\"ðŸ§¾ Number of columns: {num_columns}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deeper Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "talBuE4C5QnW"
      },
      "source": [
        "## Checking for Distribution of Target Variable\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dnJM_1MM5ZA-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+-------+\n",
            "|             label|  count|\n",
            "+------------------+-------+\n",
            "|            Benign|4108340|\n",
            "|   Malicious   C&C|     33|\n",
            "|Malicious   Attack|      3|\n",
            "|  Malicious   DDoS|1906796|\n",
            "+------------------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Register DataFrame as a SQL table\n",
        "df.createOrReplaceTempView(\"malware_data\")\n",
        "\n",
        "# Define SQL Query to count the number of rows with 0 and 1 in the target variable\n",
        "query = \"\"\"\n",
        "SELECT\n",
        "    label,\n",
        "    COUNT(*) AS count\n",
        "FROM\n",
        "    malware_data\n",
        "GROUP BY\n",
        "    label\n",
        "\"\"\"\n",
        "\n",
        "# Run the SQL query\n",
        "result = spark.sql(query)\n",
        "result.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctcKSB-xd2Vm"
      },
      "source": [
        "Roughly 2:1 ratio, mildly imbalanced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6wuYZ0OJzfC"
      },
      "source": [
        "## Summary statistics for numeric columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "7fx_4-ySJ3-b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>summary</th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>stddev</th>\n",
              "      <th>min</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>duration</th>\n",
              "      <td>6015172</td>\n",
              "      <td>4.238091014245806</td>\n",
              "      <td>36.329979153532996</td>\n",
              "      <td>2.0E-6</td>\n",
              "      <td>85755.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>orig_bytes</th>\n",
              "      <td>6015172</td>\n",
              "      <td>3.5601210492327917E8</td>\n",
              "      <td>3.4676924920430346E9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.6205577E10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>resp_bytes</th>\n",
              "      <td>6015172</td>\n",
              "      <td>5591.520989258495</td>\n",
              "      <td>1.2956898601109171E7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.17205115E10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>missed_bytes</th>\n",
              "      <td>6015172</td>\n",
              "      <td>317.33895822097855</td>\n",
              "      <td>778288.884426687</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.90881946E9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>orig_pkts</th>\n",
              "      <td>6015172</td>\n",
              "      <td>7.045628121689621</td>\n",
              "      <td>3458.6014117098734</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4216883.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>orig_ip_bytes</th>\n",
              "      <td>6015172</td>\n",
              "      <td>291.06053143617504</td>\n",
              "      <td>96841.1278582448</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.1807272E8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>resp_pkts</th>\n",
              "      <td>6015172</td>\n",
              "      <td>0.0020454942934300135</td>\n",
              "      <td>1.917404129819029</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4621.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>resp_ip_bytes</th>\n",
              "      <td>6015172</td>\n",
              "      <td>0.8743738333666934</td>\n",
              "      <td>523.3913984945711</td>\n",
              "      <td>0.0</td>\n",
              "      <td>413488.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "summary          count                   mean                stddev     min  \\\n",
              "duration       6015172      4.238091014245806    36.329979153532996  2.0E-6   \n",
              "orig_bytes     6015172   3.5601210492327917E8  3.4676924920430346E9     0.0   \n",
              "resp_bytes     6015172      5591.520989258495  1.2956898601109171E7     0.0   \n",
              "missed_bytes   6015172     317.33895822097855      778288.884426687     0.0   \n",
              "orig_pkts      6015172      7.045628121689621    3458.6014117098734     0.0   \n",
              "orig_ip_bytes  6015172     291.06053143617504      96841.1278582448     0.0   \n",
              "resp_pkts      6015172  0.0020454942934300135     1.917404129819029     0.0   \n",
              "resp_ip_bytes  6015172     0.8743738333666934     523.3913984945711     0.0   \n",
              "\n",
              "summary                  max  \n",
              "duration            85755.84  \n",
              "orig_bytes      6.6205577E10  \n",
              "resp_bytes     3.17205115E10  \n",
              "missed_bytes    1.90881946E9  \n",
              "orig_pkts          4216883.0  \n",
              "orig_ip_bytes    1.1807272E8  \n",
              "resp_pkts             4621.0  \n",
              "resp_ip_bytes       413488.0  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# convert numerical columns\n",
        "cols_to_convert = [\n",
        "    \"duration\", \"orig_bytes\", \"resp_bytes\",\n",
        "    \"missed_bytes\", \"orig_pkts\", \"orig_ip_bytes\", \"resp_pkts\", \"resp_ip_bytes\"\n",
        "]\n",
        "\n",
        "# Convert to FloatType\n",
        "for column in cols_to_convert:\n",
        "    df = df.withColumn(column, col(column).cast(FloatType()))\n",
        "\n",
        "# Descriptive stats with Pandas for nicer formatting\n",
        "desc_stats = df.describe(cols_to_convert)\n",
        "desc_stats_pd = desc_stats.toPandas().set_index('summary').T\n",
        "display(desc_stats_pd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Boxplots for numerical variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Boxplots\n",
        "# Set the number of columns in the grid\n",
        "# numeric_cols = df_split.select_dtypes(include=[\"number\"]).columns\n",
        "numeric_cols = [col_name for col_name, dtype in df.dtypes if dtype in ['int', 'double', 'float']]\n",
        "\n",
        "df_pandas = df.select(numeric_cols).toPandas()\n",
        "\n",
        "# Create subplots\n",
        "num_cols = 5\n",
        "\n",
        "# Calculate the number of rows needed based on the number of variables\n",
        "num_vars = len(numeric_cols)\n",
        "num_rows = (num_vars + num_cols - 1) // num_cols\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 10))\n",
        "\n",
        "# Flatten the axes array for easy iteration\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Iterate through each variable and create a horizontal boxplot\n",
        "for i, col_name in enumerate(numeric_cols):\n",
        "    sns.boxplot(data=df_pandas[col_name], ax=axes[i], color='skyblue')\n",
        "    axes[i].set_title(col_name)\n",
        "\n",
        "# Remove any empty subplots\n",
        "for i in range(num_vars, num_rows * num_cols):\n",
        "    fig.delaxes(axes[i])\n",
        "\n",
        "# Adjust spacing between subplots\n",
        "plt.tight_layout(rect=(0, 0, 1.5, 1.5))\n",
        "\n",
        "# Show the grid of boxplots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1Tp_JOOtNht"
      },
      "source": [
        "## Categorical Value Distibution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "rVdzZqH_tQor",
        "outputId": "5ed9247b-a7d6-4d64-d5ff-d35fe708b9ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== proto Value Counts ===\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>proto</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tcp</td>\n",
              "      <td>6013405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>udp</td>\n",
              "      <td>1745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>icmp</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  proto    count\n",
              "0   tcp  6013405\n",
              "1   udp     1745\n",
              "2  icmp       22"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== conn_state Value Counts ===\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conn_state</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>S0</td>\n",
              "      <td>4105597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RSTOS0</td>\n",
              "      <td>1841173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>OTH</td>\n",
              "      <td>65558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>REJ</td>\n",
              "      <td>1949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SF</td>\n",
              "      <td>747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>RSTO</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>RSTR</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>S2</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>S1</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>SH</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>RSTRH</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>S3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   conn_state    count\n",
              "0          S0  4105597\n",
              "1      RSTOS0  1841173\n",
              "2         OTH    65558\n",
              "3         REJ     1949\n",
              "4          SF      747\n",
              "5        RSTO       52\n",
              "6        RSTR       47\n",
              "7          S2       26\n",
              "8          S1       19\n",
              "9          SH        2\n",
              "10      RSTRH        1\n",
              "11         S3        1"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== history Value Counts ===\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>history</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>S</td>\n",
              "      <td>4104511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I</td>\n",
              "      <td>1841162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DTT</td>\n",
              "      <td>65534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sr</td>\n",
              "      <td>1948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>D</td>\n",
              "      <td>1086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>ShAdDaRRR</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>ShAfdtF</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>ShADdFf</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>HaDdAr</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>ShADad</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>62 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      history    count\n",
              "0           S  4104511\n",
              "1           I  1841162\n",
              "2         DTT    65534\n",
              "3          Sr     1948\n",
              "4           D     1086\n",
              "..        ...      ...\n",
              "57  ShAdDaRRR        1\n",
              "58    ShAfdtF        1\n",
              "59    ShADdFf        1\n",
              "60     HaDdAr        1\n",
              "61     ShADad        1\n",
              "\n",
              "[62 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "categorical_cols = [\"proto\", \"conn_state\", \"history\"]\n",
        "\n",
        "for col in categorical_cols:\n",
        "    count_df = df.groupBy(col).count().orderBy(\"count\", ascending=False).toPandas()\n",
        "    print(f\"=== {col} Value Counts ===\")\n",
        "    display(count_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cardinality of id_orig_h: 28\n",
            "Cardinality of id_orig_p: 65536\n",
            "Cardinality of id_resp_h: 4098969\n",
            "Cardinality of id_resp_p: 16\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import countDistinct\n",
        "\n",
        "# Count distinct values for each column\n",
        "cardinality = df.select(\n",
        "    countDistinct(\"id_orig_h\").alias(\"id_orig_h_cardinality\"),\n",
        "    countDistinct(\"id_orig_p\").alias(\"id_orig_p_cardinality\"),\n",
        "    countDistinct(\"id_resp_h\").alias(\"id_resp_h_cardinality\"),\n",
        "    countDistinct(\"id_resp_p\").alias(\"id_resp_p_cardinality\")\n",
        ").collect()[0]\n",
        "\n",
        "# Print the cardinality of each column\n",
        "print(f\"Cardinality of id_orig_h: {cardinality['id_orig_h_cardinality']}\")\n",
        "print(f\"Cardinality of id_orig_p: {cardinality['id_orig_p_cardinality']}\")\n",
        "print(f\"Cardinality of id_resp_h: {cardinality['id_resp_h_cardinality']}\")\n",
        "print(f\"Cardinality of id_resp_p: {cardinality['id_resp_p_cardinality']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "id_orig_h has low cardinality -> 28 so likely represent small number of internal source IPs (devices on your network)\n",
        "\n",
        "id_resp_p has low cardinality -> destination ports, likely standard ones like 80, 443, 22. tells you what kind of service was accessed. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL647mftDNjA"
      },
      "source": [
        "## Correlation matrix for numeric columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "collapsed": true,
        "id": "xtFeK3cX01Ul",
        "outputId": "bbcc9cf8-d740-477f-c64e-896f48d08fc7"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "from pyspark.sql.types import FloatType, DoubleType\n",
        "\n",
        "# Define numeric features to keep (excluding ID-like columns)\n",
        "true_numeric_cols = [\n",
        "    \"duration\", \"orig_bytes\", \"resp_bytes\",\n",
        "    \"missed_bytes\", \"orig_pkts\", \"orig_ip_bytes\", \"resp_pkts\", \"resp_ip_bytes\"\n",
        "]\n",
        "\n",
        "# Convert PySpark DataFrame to pandas DataFrame for correlation calculation\n",
        "pandas_df = df.select(numeric_cols).toPandas()\n",
        "\n",
        "# Plot correlation heatmap\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(pandas_df.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "plt.title(\"Correlation Between Numeric Features\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tM7tDTvfVa7T"
      },
      "source": [
        "(need to rewrite as we using different dataset)\n",
        "From the correlation matrix, it can be seen that:\n",
        "1. Strong Positive Correlation (Close to 1.0)\n",
        "  - `orig_pkts` and `orig_ip_bytes` are perfectly correlated (1.00), suggesting that packet count and total IP bytes from the origin are directly proportional. One of the variables could be dropped to reduce redundancy.\n",
        "  - `resp_bytes` and `resp_ip_bytes` are similarly strongly correlated (0.98). This could also mean one of the columns might be redundant.\n",
        "  - `duration` and `resp_pkts` are also strongly correlated (0.95). This makes sense as longer durations correlate with more response packets, indicating sustained communication during the duration.\n",
        "2. Weak/Negligible Correlations (Close to 0):\n",
        "  - Port numbers (`id.orig_p`, `id.resp_p`) have near-zero correlations with other features suggesting that ports do not directly influence traffic metrics (e.g. duration, bytes). This is expected as ports are identifiers, not quantitative measures.\n",
        "  - Except for its strong positive correlation to `resp_ip_bytes`, `resp_bytes` and most other features have weak/negligible correlations (0.00-0.21) possibly indicating isolated response behaviors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvu7dhOjFPyJ"
      },
      "source": [
        "## Mutual Information Scoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1wRa5nMjUwBX",
        "outputId": "df3e051a-ae0a-4079-ed24-153fae89a2a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MI for ts: 0.0007\n",
            "MI for uid: 0.0282\n",
            "MI for id_orig_h: 0.0072\n",
            "MI for id_orig_p: 0.0025\n",
            "MI for id_resp_h: 0.0278\n",
            "MI for id_resp_p: 0.0016\n",
            "MI for proto: 0.0116\n",
            "MI for duration: 0.0039\n",
            "MI for orig_bytes: 0.0024\n",
            "MI for resp_bytes: 0.0004\n",
            "MI for conn_state: 0.0230\n",
            "MI for history: 0.0176\n",
            "MI for orig_pkts: 0.0010\n",
            "MI for orig_ip_bytes: 0.0017\n",
            "MI for resp_pkts: 0.0023\n",
            "MI for resp_ip_bytes: 0.0023\n",
            "MI for detailed-label: 0.0056\n",
            "\n",
            "=== Features sorted by Mutual Information ===\n",
            "uid: 0.0282\n",
            "id_resp_h: 0.0278\n",
            "conn_state: 0.0230\n",
            "history: 0.0176\n",
            "proto: 0.0116\n",
            "id_orig_h: 0.0072\n",
            "detailed-label: 0.0056\n",
            "duration: 0.0039\n",
            "id_orig_p: 0.0025\n",
            "orig_bytes: 0.0024\n",
            "resp_pkts: 0.0023\n",
            "resp_ip_bytes: 0.0023\n",
            "orig_ip_bytes: 0.0017\n",
            "id_resp_p: 0.0016\n",
            "orig_pkts: 0.0010\n",
            "ts: 0.0007\n",
            "resp_bytes: 0.0004\n"
          ]
        }
      ],
      "source": [
        "# Discretise numerical features\n",
        "numeric_cols = [col_name for col_name, dtype in df.dtypes if dtype in ['int', 'double', 'float']]\n",
        "\n",
        "# compute mutual info\n",
        "def compute_mutual_info(df, feature_col, target_col=\"label\", n_bins=5):\n",
        "    # 5 quantile based bins per numerical col\n",
        "    if feature_col in numeric_cols:\n",
        "        df = df.withColumn(\n",
        "            f\"{feature_col}_bin\",\n",
        "            F.ntile(n_bins).over(Window.orderBy(feature_col)))\n",
        "        feature_col = f\"{feature_col}_bin\"\n",
        "\n",
        "    # Compute joint probability P(X,Y)\n",
        "    joint_prob = (\n",
        "        df.groupBy(feature_col, target_col)\n",
        "        .agg(F.count(\"*\").alias(\"count\"))\n",
        "        .withColumn(\"p_xy\", F.col(\"count\") / df.count())\n",
        "    )\n",
        "\n",
        "    # Compute marginal probabilities P(X) and P(Y)\n",
        "    p_x = joint_prob.groupBy(feature_col).agg(F.sum(\"p_xy\").alias(\"p_x\"))\n",
        "    p_y = joint_prob.groupBy(target_col).agg(F.sum(\"p_xy\").alias(\"p_y\"))\n",
        "\n",
        "    # Calculate MI terms: p_xy * log2(p_xy / (p_x * p_y))\n",
        "    mi_terms = (\n",
        "        joint_prob.join(p_x, feature_col)\n",
        "        .join(p_y, target_col)\n",
        "        .withColumn(\n",
        "            \"mi_term\",\n",
        "            F.col(\"p_xy\") * F.log2(F.col(\"p_xy\") / (F.col(\"p_x\") * F.col(\"p_y\")))\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Sum MI terms to get final score\n",
        "    mi = mi_terms.agg(F.sum(\"mi_term\").alias(\"mi\")).collect()[0][\"mi\"]\n",
        "    return float(mi) if mi else 0.0\n",
        "\n",
        "# Compute MI for all features\n",
        "df_mis = df.select(\"*\")\n",
        "mi_results = {}\n",
        "\n",
        "for feature in df_mis.columns:  # Combine all features\n",
        "    if feature == \"label\":\n",
        "        continue\n",
        "    mi_score = compute_mutual_info(df_mis, feature)\n",
        "    mi_results[feature] = mi_score\n",
        "    print(f\"MI for {feature}: {mi_score:.4f}\")\n",
        "\n",
        "# Sort the MI and display results\n",
        "sorted_mi_results = sorted(mi_results.items(), key=lambda x: x[1], reverse=True)\n",
        "print(\"\\n=== Features sorted by Mutual Information ===\")\n",
        "for feature, mi_score in sorted_mi_results:\n",
        "    print(f\"{feature}: {mi_score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXRzg0OfdxU1"
      },
      "source": [
        "(need to rewrite as we using different dataset now)\n",
        "**Interpretation of Mutual Information (MI) Scoring:**\n",
        "\n",
        "MI scoring is a measure of how much information one variable (feature) provides about another (target variable, in this case, `label`). It helps identify which features are most relevant for predicting the target, where a higher MI score means the feature is more informative for the prediction and should be prioritised for model training.\n",
        "\n",
        "From the results:\n",
        "The top 5 informative features are\n",
        "1. `uid` (0.0282)\n",
        "2. `id_resp_h` (0.0278)\n",
        "3. `conn_state` (0.0230)\n",
        "4. `history` (0.0176)\n",
        "5. `proto` (0.0116)\n",
        "\n",
        "On the other hand, the features with low/zero MI score are `resp_bytes` (0.0004) and `ts` (0.0007) which have very low MI and may have a negligible effect on prediction.\n",
        "\n",
        "**Data Leakage Issue**\n",
        "\n",
        "While `uid` has a high MI score, it could suggest that the `uid` is leaking information about the `label` as `uid` is supposed to be a unique identifier for the connection. Since it is unique per row, it has zero predictive power on new data and cannot be generalised onto new samples. This could cause the model to perform extremely well on training data but fail on unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LwH82mRclPZ"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTUT3pONMZ4x"
      },
      "source": [
        "## Engineering new Meaningful Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5RsBjD0cn3k"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, when, lit, log1p\n",
        "\n",
        "# Total Bytes\n",
        "df = df.withColumn(\"total_bytes\", col(\"orig_bytes\") + col(\"resp_bytes\"))\n",
        "\n",
        "# Byte Ratio (orig_bytes / (resp_bytes + 1))\n",
        "df = df.withColumn(\"byte_ratio\", col(\"orig_bytes\") / (col(\"resp_bytes\") + lit(1)))\n",
        "\n",
        "# Packet Ratio (orig_pkts / (resp_pkts + 1))\n",
        "df = df.withColumn(\"pkt_ratio\", col(\"orig_pkts\") / (col(\"resp_pkts\") + lit(1)))\n",
        "\n",
        "# Total Packet Count\n",
        "df = df.withColumn(\"total_pkts\", col(\"orig_pkts\") + col(\"resp_pkts\"))\n",
        "\n",
        "# Throughput = total_bytes / (duration + 1e-6)\n",
        "df = df.withColumn(\"throughput\", col(\"total_bytes\") / (col(\"duration\") + lit(1e-6)))\n",
        "\n",
        "# Data-to-packet efficiency\n",
        "df = df.withColumn(\"efficiency\", col(\"total_bytes\") / (col(\"total_pkts\") + lit(1)))\n",
        "\n",
        "df.select(\"total_bytes\", \"byte_ratio\", \"throughput\", \"is_asymmetric\").show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpbaXcxPLdjk"
      },
      "source": [
        "## Encoding of Categorical Variables:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Categorical varaiables included are History, proto, conn_state, and id_resp_p. From data exploration, cardinality of each of them are 61, 3, 11, 16 respectively.\n",
        "\n",
        "History has high cardinality so need to do frequency encoding.\n",
        "Others have relative low cardinality so one-hot encoding is feasible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84nGnJXTLpDp"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import count\n",
        "\n",
        "## Frequency Encoding of High Cardinality Features such as History\n",
        "\n",
        "# Calculate frequency counts for each category in the 'history' column\n",
        "history_counts = df.groupBy(\"history\").agg(count(\"*\").alias(\"history_count\"))\n",
        "\n",
        "# Join the frequency counts back to the original DataFrame\n",
        "df = df.join(history_counts, \"history\", \"left\")\n",
        "\n",
        "# Create a new column 'history_freq_encoded' with the frequency counts\n",
        "df = df.withColumn(\"history_freq_encoded\", col(\"history_count\"))\n",
        "\n",
        "# Optionally drop the intermediate 'history_count' column\n",
        "df = df.drop(\"history_count\")\n",
        "\n",
        "# Show the DataFrame with the frequency encoded column\n",
        "df.select(\"history\", \"history_freq_encoded\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bsqzm_ErHsbU"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
        "\n",
        "# one hot encoding of categorical features with low cardinality\n",
        "\n",
        "# Create StringIndexers for \"proto\" and \"conn_state\"\n",
        "proto_indexer = StringIndexer(inputCol=\"proto\", outputCol=\"proto_index\")\n",
        "conn_state_indexer = StringIndexer(inputCol=\"conn_state\", outputCol=\"conn_state_index\")\n",
        "port_indexer = StringIndexer(inputCol=\"id_resp_p\", outputCol=\"id_resp_p_index\")\n",
        "\n",
        "# Create OneHotEncoders for the indexed columns\n",
        "proto_encoder = OneHotEncoder(inputCol=\"proto_index\", outputCol=\"proto_encoded\")\n",
        "conn_state_encoder = OneHotEncoder(inputCol=\"conn_state_index\", outputCol=\"conn_state_encoded\")\n",
        "port_encoder = OneHotEncoder(inputCol=\"id_resp_p_index\", outputCol=\"id_resp_p_encoded\")\n",
        "\n",
        "# Fit and transform the DataFrame\n",
        "df = proto_indexer.fit(df).transform(df)\n",
        "df = conn_state_indexer.fit(df).transform(df)\n",
        "df = port_indexer.fit(df).transform(df)\n",
        "\n",
        "df = proto_encoder.fit(df).transform(df)\n",
        "df = conn_state_encoder.fit(df).transform(df)\n",
        "df = port_encoder.fit(df).transform(df)\n",
        "\n",
        "# Show the DataFrame with the one-hot encoded columns\n",
        "df.select(\"proto\", \"proto_index\", \"proto_encoded\", \"conn_state\", \"conn_state_index\", \"conn_state_encoded\", \"id_resp_p\", \"id_resp_p_index\", \"id_resp_p_encoded\").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIw_teB0ITOI"
      },
      "source": [
        "## Scaling of Numerical Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHA9IuSTIh0Q"
      },
      "outputs": [],
      "source": [
        "# Assuming 'df' is your DataFrame and the features are already created\n",
        "\n",
        "# Define the columns to be scaled\n",
        "cols_to_scale = [\"duration\", \"byte_ratio\", \"pkt_ratio\", \"total_bytes\", \"total_pkts\", \"efficiency\", \"throughput\"]\n",
        "\n",
        "# Create a VectorAssembler to combine the features into a single vector column\n",
        "assembler = VectorAssembler(inputCols=cols_to_scale, outputCol=\"features_unscaled\")\n",
        "df = assembler.transform(df)\n",
        "\n",
        "# Create a StandardScaler to scale the features\n",
        "scaler = StandardScaler(inputCol=\"features_unscaled\", outputCol=\"features_scaled\", withStd=True, withMean=True)\n",
        "scaler_model = scaler.fit(df)\n",
        "df = scaler_model.transform(df)\n",
        "\n",
        "# Show the scaled features\n",
        "df.select(\"features_scaled\").show(5, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0I8dV6gL7zN"
      },
      "source": [
        "## Assemble Features for Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uL1kqhTL_7d"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "final_features = [\n",
        "    \"features_scaled\",           # your numeric features\n",
        "    \"proto_encoded\",             # one-hot categorical\n",
        "    \"conn_state_encoded\",\n",
        "    \"history_encoded\",\n",
        "    \"id_resp_p_encoded\"\n",
        "]\n",
        "\n",
        "assembler = VectorAssembler(inputCols=final_features, outputCol=\"features\")\n",
        "df_model = assembler.transform(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R43x7rDi5sF8"
      },
      "source": [
        "# Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCclzB3TMgX6"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqGI5rYwM39F"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_index\")\n",
        "df_model = label_indexer.fit(df_model).transform(df_model)\n",
        "\n",
        "\n",
        "lr = LogisticRegression(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label_index\",\n",
        "    maxIter=20,\n",
        "    regParam=0.1,\n",
        "    elasticNetParam=0.0  # L2 regularization (Ridge)\n",
        ")\n",
        "\n",
        "lr_model = lr.fit(df_model)\n",
        "\n",
        "predictions = lr_model.transform(df_model)\n",
        "predictions.select(\"label\", \"label_index\", \"probability\", \"prediction\").show(5, truncate=False)\n",
        "\n",
        "\n",
        "evaluator_auc = BinaryClassificationEvaluator(\n",
        "    labelCol=\"label_index\",\n",
        "    rawPredictionCol=\"rawPrediction\",\n",
        "    metricName=\"areaUnderROC\"\n",
        ")\n",
        "\n",
        "auc = evaluator_auc.evaluate(predictions)\n",
        "print(f\"ðŸ”¥ ROC-AUC: {auc:.4f}\")\n",
        "\n",
        "\n",
        "#confusion matrix:\n",
        "predictions.groupBy(\"label_index\", \"prediction\").count().show()\n",
        "\n",
        "train, test = df_model.randomSplit([0.8, 0.2], seed=42)\n",
        "lr_model = lr.fit(train)\n",
        "predictions = lr_model.transform(test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phW0E54BMjuc"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NxND86oM56v"
      },
      "outputs": [],
      "source": [
        "# to be filled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLBMnQuGMwBZ"
      },
      "source": [
        "## Gradient Boosted Trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sy5xHJXsM7Nt"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# gbt regressor\n",
        "gbt = GBTClassifier(featuresCol=\"features\",\n",
        "                    labelCol=\"label_index\", \n",
        "                    maxIter=50, \n",
        "                    maxDepth=5)\n",
        "# gbt pipeline\n",
        "gbt_pipeline = Pipeline(stages=[\n",
        "    label_indexer,\n",
        "    encoder, # insert encoders !!\n",
        "    assembler,\n",
        "    gbt\n",
        "])\n",
        "# train the model\n",
        "gbt_model = gbt_pipeline.fit(train)\n",
        "\n",
        "# Make predictions with Gradient Boosted Tree\n",
        "gbt_predictions = gbt_model.transform(test)\n",
        "\n",
        "# evaluate AUC using BinaryClassificationEvaluator\n",
        "evaluator_auc = BinaryClassificationEvaluator(\n",
        "    labelCol=\"label_index\",\n",
        "    rawPredictionCol=\"rawPrediction\",  # GBT outputs rawPrediction\n",
        "    metricName=\"areaUnderROC\"\n",
        ")\n",
        "\n",
        "gbt_auc = evaluator_auc.evaluate(gbt_predictions)\n",
        "print(f\"GBT AUC: {gbt_auc:.4f}\")\n",
        "\n",
        "evaluator_acc = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label_index\", \n",
        "    predictionCol=\"prediction\", \n",
        "    metricName=\"accuracy\"\n",
        ")\n",
        "\n",
        "gbt_accuracy = evaluator_acc.evaluate(gbt_predictions)\n",
        "print(f\"Accuracy: {gbt_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lehy1GtUM1bq"
      },
      "source": [
        "## MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwdyBCqcM81h"
      },
      "outputs": [],
      "source": [
        "# to be filled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_f4FngJ5tOD"
      },
      "source": [
        "# Hyperparameter Tuning & Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHoLi5yvNHjz"
      },
      "outputs": [],
      "source": [
        "# to be done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVSpNYqJ52bF"
      },
      "source": [
        "# Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phT0fenb56Nq"
      },
      "source": [
        "write interpretations here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv-spark",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
